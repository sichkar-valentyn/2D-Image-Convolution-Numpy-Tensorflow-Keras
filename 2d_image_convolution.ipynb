{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Course:  Convolutional Neural Networks for Image Classification\n",
    "\n",
    "## &nbsp; â›©ï¸ Section-9\n",
    "### &nbsp; &nbsp; ðŸŽ›ï¸ 2D Image Convolution: Numpy, Tensorflow, Keras\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**Description:**  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*Explained theory on how **2D Convolutional Layer** creates **feature maps**.*  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*Instructions on how to use **Tensorflow** and **Keras** functions **â€˜get_weights()â€™** and **â€˜set_weights()â€™**.*  \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**File:** *2d_image_convolution.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Algorithm:\n",
    "\n",
    "**âœ”ï¸ Step 1:** Load input image  \n",
    "**âœ”ï¸ Step 2:** Set up filters for **edge detection**  \n",
    "**âœ”ï¸ Step 3:** Apply 2D convolution by **'for' loops**  \n",
    "**âœ”ï¸ Step 4:** Apply 2D convolution by **Tensorflow**  \n",
    "**âœ”ï¸ Step 5:** Apply 2D convolution by **Keras**  \n",
    "**âœ”ï¸ Step 6:** Compute **time** spent for 2D convolution  \n",
    "**âœ”ï¸ Step 7:** Implement 2D convolution on **video**  \n",
    "  \n",
    "  \n",
    "### ðŸŽ¯ **Result:**  \n",
    "**âœ… Plot** of GRAY images with detected edges  \n",
    "**âœ… Time** spent for 2D convolution by different approaches  \n",
    "**âœ… Video** of GRAY object with detected edge  \n",
    "**âœ… Video** of RGB object with bounding box and label  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¥ Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Importing library to see calculation progress inside loops in Real Time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Importing Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Importing Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "# Importing timer\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Hint: to print emoji via Unicode, replace '+' with '000' and add prefix '\\'\n",
    "# For instance, emoji with Unicode 'U+1F44C' can be printed as '\\U0001F44C'\n",
    "print('Libraries are successfully loaded ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ–¼ï¸ Step 1: Load input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Content:**  \n",
    "  \n",
    "ðŸ’  1.**1** **Load** input image by OpenCV  \n",
    "ðŸ’  1.**2** **Visualize** input image  \n",
    "ðŸ’  1.**3** **Prepare** input to the 2D Convolutional Layer  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load input image by OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input image by OpenCV library\n",
    "# In this way image is opened as Numpy array\n",
    "\n",
    "# (!) OpenCV by default reads images in BGR format (order of channels)\n",
    "# (!) On Windows, the path might look like following:\n",
    "# r'images\\cat.png'\n",
    "# or:\n",
    "# 'images\\\\cat.png'\n",
    "image_BGR = cv2.imread('images/cat.png')\n",
    "\n",
    "\n",
    "# Converting image to RGB by OpenCV function\n",
    "image_RGB = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_GRAY = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing type and shapes of loaded and converted image\n",
    "print('Type of image_BGR is   :', type(image_BGR))\n",
    "print('Shape of image_RGB is  :', image_RGB.shape)\n",
    "print('Shape of image_GRAY is :', image_GRAY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualize input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Defining a figure object with number of needed subplots\n",
    "figure, ax = plt.subplots(nrows=1, ncols=2)\n",
    "# ax is a (2,) Numpy array and to access specific subplot we call it by ax[0]\n",
    "\n",
    "\n",
    "# Adjusting 1st column with RGB image\n",
    "ax[0].imshow(image_RGB)\n",
    "\n",
    "\n",
    "# Adjusting 2nd column with GRAY image\n",
    "ax[1].imshow(image_GRAY, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes to all subplots\n",
    "for i in range(2):\n",
    "    ax[i].axes.xaxis.set_ticks([])\n",
    "    ax[i].axes.yaxis.set_ticks([])\n",
    "\n",
    "\n",
    "# Giving names to subplots along X axis\n",
    "ax[0].set_xlabel('RGB', fontsize=18)\n",
    "ax[1].set_xlabel('GRAY', fontsize=18)\n",
    "\n",
    "\n",
    "# Moving subplots names to the top positions\n",
    "ax[0].xaxis.set_label_position('top')\n",
    "ax[1].xaxis.set_label_position('top')\n",
    "\n",
    "\n",
    "# Adjusting distance between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Prepare input to the 2D Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check points\n",
    "# Showing shapes of GRAY and RGB images\n",
    "print('Shape of image_GRAY is   :', image_GRAY.shape)\n",
    "print('Shape of image_RGB is    :', image_RGB.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Reshaping GRAY image to get following: (batch size, rows, columns, channels)\n",
    "x_input_GRAY = image_GRAY.reshape(1, image_GRAY.shape[0], image_GRAY.shape[1], 1).astype(np.float32)\n",
    "\n",
    "\n",
    "# Reshaping RGB image to get following: (batch size, rows, columns, channels)\n",
    "x_input_RGB = image_RGB.reshape(1, image_RGB.shape[0], image_RGB.shape[1], 3).astype(np.float32)\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of reshaped images\n",
    "print('Shape of x_input_GRAY is :', x_input_GRAY.shape)\n",
    "print('Shape of x_input_RGB is  :', x_input_RGB.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ðŸŒ€ Step 2: Set up filters for edge detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Content:**  \n",
    "  \n",
    "ðŸ’  2.**1** **Define** 1-channeled common 3x3 filters (kernels) for edge detection  \n",
    "ðŸ’  2.**2** **Extend** 1-channeled filters to 3-channeled with identical channels  \n",
    "ðŸ’  2.**3** **Assemble** 1-channeled filters into united array  \n",
    "ðŸ’  2.**4** **Assemble** 3-channeled filters into united array  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define 1-channeled common 3x3 filters (kernels) for edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobel filter to detect vertical changes on image\n",
    "f1 = np.array([[1, 0, -1],\n",
    "               [2, 0, -2],\n",
    "               [1, 0, -1]])\n",
    "\n",
    "\n",
    "# Laplacian filter to detect regions with different brightness on image\n",
    "f2 = np.array([[0, 1, 0], \n",
    "               [1, -4, 1], \n",
    "               [0, 1, 0]])\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of the filters\n",
    "print('Shape of the Sobel filter     is :', f1.shape)\n",
    "print('Shape of the Laplacian filter is :', f2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extend 1-channeled filters to 3-channeled with identical channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting 3 channels for the Sobel filter\n",
    "f1_3 = np.array((f1, f1, f1))\n",
    "\n",
    "\n",
    "# Moving channels dimension to the last position: (height, width, channels)\n",
    "f1_3 = f1_3.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing one channel and shape of the 3-channeled filter\n",
    "print(f1_3[:, :, 0])\n",
    "print()\n",
    "print(f1_3.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting 3 channels for the Laplacian filter\n",
    "f2_3 = np.array((f2, f2, f2))\n",
    "\n",
    "\n",
    "# Moving channels dimension to the last position: (height, width, channels)\n",
    "f2_3 = f2_3.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing one channel and shape of the 3-channeled filter\n",
    "print(f2_3[:, :, 0])\n",
    "print()\n",
    "print(f2_3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Assemble 1-channeled filters into united array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling 1-channeled filters into one, united array\n",
    "f11 = np.array((f1, f2))\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the united array\n",
    "print(f11.shape)  # (2, 3, 3)\n",
    "\n",
    "\n",
    "# Reshaping united array to get following: (filters, height, width, channels)\n",
    "f11 = f11.reshape(f11.shape + tuple([1]))\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the united array\n",
    "print(f11.shape)  # (2, 3, 3, 1)\n",
    "\n",
    "\n",
    "# Moving filters dimension to the last position: (height, width, channels, filters)\n",
    "f11 = f11.transpose(1, 2, 3, 0)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the united array\n",
    "print(f11.shape)  # (3, 3, 1, 2)\n",
    "print()\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing one channel of the 2 filters from united array\n",
    "print(f11[:, :, 0, 0])\n",
    "print()\n",
    "print(f11[:, :, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Assemble 3-channeled filters into united array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assembling 3-channeled filters into one, united array\n",
    "f22 = np.array((f1_3, f2_3))\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the united array\n",
    "print(f22.shape)  # (2, 3, 3, 3)\n",
    "\n",
    "\n",
    "# Moving filters dimension to the last position: (height, width, channels, filters)\n",
    "f22 = f22.transpose(1, 2, 3, 0)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the united array\n",
    "print(f22.shape)  # (3, 3, 3, 2)\n",
    "print()\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing one channel of the 2 filters from united array\n",
    "print(f22[:, :, 0, 0])\n",
    "print()\n",
    "print(f22[:, :, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  âž° Step 3: Apply 2D convolution by 'for' loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Content:**  \n",
    "  \n",
    "ðŸ’  3.**1** **Set up** hyperparameters  \n",
    "ðŸ’  3.**2** **Apply** pad frame to GRAY input  \n",
    "ðŸ’  3.**3** **Apply** 2D convolution by 'for' loops to GRAY input  \n",
    "ðŸ’  3.**4** **Exclude** non-needed values (less than 0 and more than 255)  \n",
    "ðŸ’  3.**5** **Visualize** obtained feature map  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set up hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing hyperparameters for convolution**  \n",
    "To get convolved image with the same dimension as input image, it is needed to set following:  \n",
    "  \n",
    "> <font color='#00a1e6' size=3>$f_{size} = 3$</font>, filter size (kernel), width and height are equal;  \n",
    "> <font color='#00a1e6' size=3>$stride = 1$</font>, stride (step) for sliding;  \n",
    "> <font color='#00a1e6' size=3>$pad = 1$</font>, pad to process boundaries (zero valued frame around image).  \n",
    "\n",
    "Output image dimension is calculated by following equations:  \n",
    "> <font color='#00a1e6' size=4>$height_{output} = \\frac {height_{input} - f_{size} + 2 * pad}{step} + 1$</font>  \n",
    ">  \n",
    "> <font color='#00a1e6' size=4>$width_{output} = \\frac {width_{input} - f_{size} + 2 * pad}{step} + 1$</font>\n",
    "  \n",
    "For instance, dimension of the input GRAY image is 853x1280 (height and width), then dimension of the output image will be as following:  \n",
    "> <font color='#00a1e6' size=4>$height_{output} = \\frac {853 - 3 + 2 * 1}{1} + 1 = 853$</font>  \n",
    ">  \n",
    "> <font color='#00a1e6' size=4>$width_{output} = \\frac {1280 - 3 + 2 * 1}{1} + 1 = 1280$</font>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Apply pad frame to GRAY input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying to input GRAY image pad frame with zero values to process boundaries\n",
    "# Using Numpy method 'pad'\n",
    "image_GRAY_pad = np.pad(image_GRAY, (1, 1), mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of input GRAY image and its version with pad frame\n",
    "print('Shape of image_GRAY is : ', image_GRAY.shape)\n",
    "print('With pad               : ', image_GRAY_pad.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply 2D convolution by 'for' loops to GRAY input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing zero valued array for convolved output image\n",
    "# Dimension is the same with input image according to chosen hyperparameters\n",
    "# Passing as argument tuple with needed shape\n",
    "output = np.zeros(image_GRAY.shape)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the output\n",
    "print('Shape of the output is : ', output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing convolution operation to GRAY image\n",
    "# Sliding through entire input image (that is with pad frame) by Sobel filter\n",
    "# Wrapping the loop with 'tqdm' in order to see progress in Real Time\n",
    "for i in tqdm(range(image_GRAY_pad.shape[0] - 2)):\n",
    "    for j in range(image_GRAY_pad.shape[1] - 2):\n",
    "        # Extracting (slicing) a 3x3 patch (the same size with filter)\n",
    "        # from input image with pad frame\n",
    "        patch = image_GRAY_pad[i:i+3, j:j+3]\n",
    "\n",
    "        # Applying elementwise multiplication and summation -\n",
    "        # this is convolution operation\n",
    "        # When we use '*' with matrices, then elementwise multiplication\n",
    "        # will be applied\n",
    "\n",
    "        # With Sobel filter\n",
    "        output[i, j] = np.sum(patch * f1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Exclude non-needed values (less than 0 and more than 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To exclude values that are less than 0 and more than 255,\n",
    "# Numpy function 'clip' is applied\n",
    "# It keeps values of Numpy array in the given range\n",
    "# And it replaces non-needed values with boundary numbers\n",
    "output = np.clip(output, 0, 255)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualize obtained feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Creating a plot with convolved image\n",
    "plt.imshow(output, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ðŸ”† Step 4: Apply 2D convolution by Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Content:**  \n",
    "  \n",
    "ðŸ’  4.**1** **Implement** *kernel_initializer*  \n",
    "ðŸ’  4.**2** What if **bias** is **True?**  \n",
    "ðŸ’  4.**3** **Implement** *weights=[]*  \n",
    "ðŸ’  4.**4** What if **input** image is **RGB?**  \n",
    "ðŸ’  4.**5** How to use **set of filters** in one layer?  \n",
    "ðŸ’  4.**6** Visualize **final results**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Implement kernel_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Sub-Content:**  \n",
    "  \n",
    "ðŸ’  4.1.**1** **Initialize** 2D Convolutional Layer by Tensorflow for GRAY input  \n",
    "ðŸ’  4.1.**2** **Pass** GRAY input through Tensorflow Conv2D layer  \n",
    "ðŸ’  4.1.**3** **Slice** from the output just feature map  \n",
    "ðŸ’  4.1.**4** **Exclude** non-needed values (less than 0 and more than 255)  \n",
    "ðŸ’  4.1.**5** **Visualize** obtained feature map  \n",
    "ðŸ’  4.1.**6** **Get** current weights  \n",
    "ðŸ’  4.1.**7** **Set up** new weights  \n",
    "ðŸ’  4.1.**8** **Pass** GRAY input through Tensorflow Conv2D layer with updated weights  \n",
    "ðŸ’  4.1.**9** **Visualize** newly obtained feature map  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Initialize 2D Convolutional Layer by Tensorflow for GRAY input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for GRAY input\n",
    "layer = tf.keras.layers.Conv2D(filters=1,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_GRAY.shape[1:],\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f1))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Pass GRAY input through Tensorflow Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the initialized Conv2D layer\n",
    "output = layer(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n')\n",
    "print('\\n' + 'Data type of the output is :', type(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Slice from the output just feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Slicing from the output just feature map\n",
    "# Converting output Tensor into Numpy array\n",
    "output = np.array(output[0, :, :, 0])\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Shape of the output is :', output.shape)\n",
    "print('\\n')\n",
    "print('\\n' + 'Data type of the output is :', type(output))\n",
    "print('\\n')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Exclude non-needed values (less than 0 and more than 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To exclude values that are less than 0 and more than 255,\n",
    "# Numpy function 'clip' is applied\n",
    "# It keeps values of Numpy array in the given range\n",
    "# And it replaces non-needed values with boundary numbers\n",
    "output = np.clip(output, 0, 255)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Visualize obtained feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Creating a plot with convolved image\n",
    "plt.imshow(output, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 Get current weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting type of the container that holds weights\n",
    "print('Datatype of the container that holds weights is :', type(layer.get_weights()))  # list\n",
    "print('Datatype of the container that holds weights is :', type(layer.weights))        # list\n",
    "print()\n",
    "\n",
    "\n",
    "# Getting lengths of the list that holds weights\n",
    "print('Lengths of the list that holds weights is       :', len(layer.get_weights()))   # 1\n",
    "print('Lengths of the list that holds weights is       :', len(layer.weights))         # 1\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shape of the weights\n",
    "print(layer.get_weights()[0].shape)  # (3, 3, 1, 1)\n",
    "print(layer.weights[0].shape)        # (3, 3, 1, 1)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Getting type of weights themselves\n",
    "print(type(layer.get_weights()[0]))  # Numpy array\n",
    "print(type(layer.weights[0]))        # tf.Tensor\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing one channel of the weights themselves\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "print(layer.weights[0][:, :, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7 Set up new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing shape of the pre-defined filter\n",
    "print(f2.shape)\n",
    "\n",
    "\n",
    "# Reshaping filter to get following: (height, width, channels, filters)\n",
    "f_new = f2.reshape(3, 3, 1, 1)\n",
    "\n",
    "\n",
    "# Showing shape of the updated filter\n",
    "print(f_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one channel of the current weights\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "# Setting up new weights\n",
    "layer.set_weights([f_new])\n",
    "\n",
    "\n",
    "# Showing one channel of the updated weights\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.8 Pass GRAY input through Tensorflow Conv2D layer with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the Conv2D layer with new weights\n",
    "output = layer(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature map\n",
    "# Converting output Tensor into Numpy array\n",
    "output = np.array(output[0, :, :, 0])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output = np.clip(output, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n' + 'Feature map is successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.9 Visualize newly obtained feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Creating a plot with convolved image\n",
    "plt.imshow(output, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 What if bias is True?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Sub-Content:**  \n",
    "  \n",
    "ðŸ’  4.2.**1** **Initialize** 2D Convolutional Layer by Tensorflow for GRAY input  \n",
    "ðŸ’  4.2.**2** **Pass** GRAY input through Tensorflow Conv2D layer  \n",
    "ðŸ’  4.2.**3** **Get** current weights and bias  \n",
    "ðŸ’  4.2.**4** **Set up** new weights and bias  \n",
    "ðŸ’  4.2.**5** **Pass** GRAY input through Tensorflow Conv2D layer with updated weights and bias  \n",
    "ðŸ’  4.2.**6** **Visualize** newly obtained feature map  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Initialize 2D Convolutional Layer by Tensorflow for GRAY input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for GRAY input\n",
    "layer = tf.keras.layers.Conv2D(filters=1,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_GRAY.shape[1:],\n",
    "                               use_bias=True,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f1))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Pass GRAY input through Tensorflow Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the initialized Conv2D layer\n",
    "output = layer(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Get current weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting type of the container that holds weights and bias\n",
    "print('Datatype of the container that holds weights and bias is :', type(layer.get_weights()))\n",
    "\n",
    "\n",
    "# Getting lengths of the list that holds weights and bias\n",
    "print('Lengths of the list that holds weights and bias is       :', len(layer.get_weights()))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shape of the weights\n",
    "print(layer.get_weights()[0].shape)  # (3, 3, 1, 1)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing one channel of the weights themselves\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shape of the bias\n",
    "print(layer.get_weights()[1].shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing bias itself\n",
    "print(layer.get_weights()[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Set up new weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing shape of the pre-defined filter\n",
    "print(f2.shape)\n",
    "\n",
    "\n",
    "# Reshaping filter to get following: (height, width, channels, filters)\n",
    "f_new = f2.reshape(3, 3, 1, 1)\n",
    "\n",
    "\n",
    "# Showing shape of the updated filter\n",
    "print(f_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining array for bias\n",
    "b = np.array([0.])\n",
    "\n",
    "\n",
    "# Showing shape of the bias\n",
    "print(b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one channel of the current weights\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "# Setting up new weights and bias\n",
    "layer.set_weights([f_new, b])\n",
    "\n",
    "\n",
    "# Showing one channel of the updated weights\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 Pass GRAY input through Tensorflow Conv2D layer with updated weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the Conv2D layer with new weights\n",
    "output = layer(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature map\n",
    "# Converting output Tensor into Numpy array\n",
    "output = np.array(output[0, :, :, 0])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output = np.clip(output, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n' + 'Feature map is successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6 Visualize newly obtained feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Creating a plot with convolved image\n",
    "plt.imshow(output, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Implement weights=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Sub-Content:**  \n",
    "  \n",
    "ðŸ’  4.3.**1** **Set** up weights  \n",
    "ðŸ’  4.3.**2** **Initialize** 2D Convolutional Layer by Tensorflow for GRAY input  \n",
    "ðŸ’  4.3.**3** **Pass** GRAY input through Tensorflow Conv2D layer  \n",
    "ðŸ’  4.3.**4** **Visualize** obtained feature map  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Set up weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing shape of the pre-defined filter\n",
    "print(f1.shape)\n",
    "\n",
    "\n",
    "# Reshaping filter to get following: (height, width, channels, filters)\n",
    "f_new = f1.reshape(3, 3, 1, 1)\n",
    "\n",
    "\n",
    "# Showing shape of the updated filter\n",
    "print(f_new.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Initialize 2D Convolutional Layer by Tensorflow for GRAY input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for GRAY input\n",
    "layer = tf.keras.layers.Conv2D(filters=1,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_GRAY.shape[1:],\n",
    "                               use_bias=False,\n",
    "                               weights=[f_new])\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Pass GRAY input through Tensorflow Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the Conv2D layer\n",
    "output = layer(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature map\n",
    "# Converting output Tensor into Numpy array\n",
    "output = np.array(output[0, :, :, 0])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output = np.clip(output, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n' + 'Feature map is successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Visualize obtained feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Creating a plot with convolved image\n",
    "plt.imshow(output, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 What if input image is RGB?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Sub-Content:**  \n",
    "  \n",
    "ðŸ’  4.4.**1** **Initialize** 2D Convolutional Layer by Tensorflow for RGB input  \n",
    "ðŸ’  4.4.**2** **Pass** RGB input through Tensorflow Conv2D layer  \n",
    "ðŸ’  4.4.**3** **Get** current weights  \n",
    "ðŸ’  4.4.**4** **Set up** new weights  \n",
    "ðŸ’  4.4.**5** **Pass** RGB input through Tensorflow Conv2D layer with updated weights  \n",
    "ðŸ’  4.4.**6** **Visualize** obtained feature map  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Initialize 2D Convolutional Layer by Tensorflow for RGB input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for RGB input\n",
    "layer = tf.keras.layers.Conv2D(filters=1,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_RGB.shape[1:],\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f1_3))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Pass RGB input through Tensorflow Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing RGB input to the initialized Conv2D layer\n",
    "output = layer(x_input_RGB)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Get current weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting type of the container that holds weights\n",
    "print('Datatype of the container that holds weights is :', type(layer.get_weights()))\n",
    "\n",
    "\n",
    "# Getting lengths of the list that holds weights\n",
    "print('Lengths of the list that holds weights is       :', len(layer.get_weights()))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shape of the weights\n",
    "print(layer.get_weights()[0].shape)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Showing one channel of the weights themselves\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Set up new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing shape of the pre-defined filter\n",
    "print(f2_3.shape)\n",
    "\n",
    "\n",
    "# Reshaping filter to get following: (height, width, channels, filters)\n",
    "f_new = f2_3.reshape(3, 3, 3, 1)\n",
    "\n",
    "\n",
    "# Showing shape of the updated filter\n",
    "print(f_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one channel of the current weights\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "# Setting up new weights\n",
    "layer.set_weights([f_new])\n",
    "\n",
    "\n",
    "# Showing one channel of the updated weights\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.5 Pass RGB input through Tensorflow Conv2D layer with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing RGB input to the Conv2D layer with new weights\n",
    "output = layer(x_input_RGB)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature map\n",
    "# Converting output Tensor into Numpy array\n",
    "output = np.array(output[0, :, :, 0])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output = np.clip(output, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n' + 'Feature map is successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.6 Visualize obtained feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Creating a plot with convolved image\n",
    "plt.imshow(output, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 How to use set of filters in one layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Sub-Content:**  \n",
    "  \n",
    "ðŸ’  4.5.**1** **Initialize** 2D Convolutional Layer by Tensorflow for **GRAY input** and set of filters  \n",
    "ðŸ’  4.5.**2** **Pass** GRAY input through Tensorflow Conv2D layer with set of filters  \n",
    "ðŸ’  4.5.**3** **Get** current weights for GRAY input  \n",
    "ðŸ’  4.5.**4** **Set up** new weights for GRAY input  \n",
    "ðŸ’  4.5.**5** **Pass GRAY** input through Tensorflow Conv2D layer with updated weights  \n",
    "ðŸ’  4.5.**6** **Visualize** obtained feature maps for GRAY input  \n",
    "ðŸ’  4.5.**7** **Initialize** 2D Convolutional Layer by Tensorflow for **RGB input** and set of filters  \n",
    "ðŸ’  4.5.**8** **Pass** RGB input through Tensorflow Conv2D layer with set of filters  \n",
    "ðŸ’  4.5.**9** **Get** current weights for RGB input  \n",
    "ðŸ’  4.5.**10** **Set up** new weights for RGB input  \n",
    "ðŸ’  4.5.**11** **Pass RGB** input through Tensorflow Conv2D layer with updated weights  \n",
    "ðŸ’  4.5.**12** **Visualize** obtained feature maps for RGB input  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Initialize 2D Convolutional Layer by Tensorflow for GRAY input and set of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for GRAY input and set of filters\n",
    "layer = tf.keras.layers.Conv2D(filters=2,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_GRAY.shape[1:],\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f11))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Pass GRAY input through Tensorflow Conv2D layer with set of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the initialized Conv2D layer with set of filters\n",
    "output = layer(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Get current weights for GRAY input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting type of the container that holds weights\n",
    "print('Datatype of the container that holds weights is :', type(layer.get_weights()))\n",
    "\n",
    "\n",
    "# Getting lengths of the list that holds weights\n",
    "print('Lengths of the list that holds weights is       :', len(layer.get_weights()))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shape of the weights\n",
    "print(layer.get_weights()[0].shape)  # (3, 3, 1, 2)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Showing one channel of the 2 weights themselves\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print('\\n')\n",
    "print(layer.get_weights()[0][:, :, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 Set up new weights for GRAY input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing shape of the pre-defined GRAY set of filters\n",
    "print(f11.shape)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one channel of the current weights\n",
    "print('Originally defined weights:\\n')\n",
    "print(f11[:, :, 0, 0])\n",
    "print()\n",
    "print(f11[:, :, 0, 1])\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Setting up new weights\n",
    "# Swapping them\n",
    "f11[:, :, 0, 0],  f11[:, :, 0, 1] = np.copy(f11[:, :, 0, 1]),  np.copy(f11[:, :, 0, 0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Showing one channel of the updated weights\n",
    "print('Weights after swapping:\\n')\n",
    "print(f11[:, :, 0, 0])\n",
    "print()\n",
    "print(f11[:, :, 0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one channel of the current weights\n",
    "print('Initialized weights:\\n')\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "print(layer.get_weights()[0][:, :, 0, 1])\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Setting up new weights\n",
    "layer.set_weights([f11])\n",
    "\n",
    "\n",
    "# Showing one channel of the updated weights\n",
    "print('Updated weights:\\n')\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "print(layer.get_weights()[0][:, :, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.5 Pass GRAY input through Tensorflow Conv2D layer with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the Conv2D layer with new weights\n",
    "output = layer(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature maps\n",
    "# Converting output Tensor into Numpy array\n",
    "output = np.array(output[0, :, :, :])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output = np.clip(output, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n' + 'Feature maps are successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.6 Visualize obtained feature maps for GRAY input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Defining a figure object with number of needed subplots\n",
    "figure, ax = plt.subplots(nrows=1, ncols=2)\n",
    "# ax is a (2,) Numpy array and to access specific subplot we call it by ax[0]\n",
    "\n",
    "\n",
    "# Adjusting 1st column\n",
    "ax[0].imshow(output[:, :, 0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Adjusting 2nd column\n",
    "ax[1].imshow(output[:, :, 1], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes to all subplots\n",
    "for i in range(2):\n",
    "    ax[i].axes.xaxis.set_ticks([])\n",
    "    ax[i].axes.yaxis.set_ticks([])\n",
    "\n",
    "\n",
    "# Giving names to subplots along X axis\n",
    "ax[0].set_xlabel('Laplacian', fontsize=18)\n",
    "ax[1].set_xlabel('Sobel', fontsize=18)\n",
    "\n",
    "\n",
    "# Moving subplots names to the top positions\n",
    "ax[0].xaxis.set_label_position('top')\n",
    "ax[1].xaxis.set_label_position('top')\n",
    "\n",
    "\n",
    "# Adjusting distance between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.7 Initialize 2D Convolutional Layer by Tensorflow for RGB input and set of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for GRAY input and set of filters\n",
    "layer = tf.keras.layers.Conv2D(filters=2,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_RGB.shape[1:],\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f22))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.8 Pass RGB input through Tensorflow Conv2D layer with set of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing RGB input to the initialized Conv2D layer with set of filters\n",
    "output = layer(x_input_RGB)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.9 Get current weights for RGB input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting type of the container that holds weights\n",
    "print('Datatype of the container that holds weights is :', type(layer.get_weights()))\n",
    "\n",
    "\n",
    "# Getting lengths of the list that holds weights\n",
    "print('Lengths of the list that holds weights is       :', len(layer.get_weights()))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shape of the weights\n",
    "print(layer.get_weights()[0].shape)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Showing one channel of the 2 weights themselves\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print('\\n')\n",
    "print(layer.get_weights()[0][:, :, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.10 Set up new weights for RGB input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing shape of the pre-defined RGB set of filters\n",
    "print(f22.shape)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one channel of the current weights\n",
    "print('Originally defined weights:\\n')\n",
    "print(f22[:, :, 0, 0])\n",
    "print()\n",
    "print(f22[:, :, 0, 1])\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Setting up new weights\n",
    "# Swapping them\n",
    "f22[:, :, :, 0],  f22[:, :, :, 1] = np.copy(f22[:, :, :, 1]),  np.copy(f22[:, :, :, 0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Showing one channel of the updated weights\n",
    "print('Weights after swapping:\\n')\n",
    "print(f22[:, :, 0, 0])\n",
    "print()\n",
    "print(f22[:, :, 0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one channel of the current weights\n",
    "print('Initialized weights:\\n')\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "print(layer.get_weights()[0][:, :, 0, 1])\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Setting up new weights\n",
    "layer.set_weights([f22])\n",
    "\n",
    "\n",
    "# Showing one channel of the updated weights\n",
    "print('Updated weights:\\n')\n",
    "print(layer.get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "print(layer.get_weights()[0][:, :, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.11 Pass RGB input through Tensorflow Conv2D layer with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing RGB input to the Conv2D layer with new weights\n",
    "output = layer(x_input_RGB)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature maps\n",
    "# Converting output Tensor into Numpy array\n",
    "output = np.array(output[0, :, :, :])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output = np.clip(output, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output.shape)\n",
    "print('\\n' + 'Feature maps are successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.12 Visualize obtained feature maps for RGB input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "# Defining a figure object with number of needed subplots\n",
    "figure, ax = plt.subplots(nrows=1, ncols=2)\n",
    "# ax is a (2,) Numpy array and to access specific subplot we call it by ax[0]\n",
    "\n",
    "\n",
    "# Adjusting 1st column\n",
    "ax[0].imshow(output[:, :, 0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Adjusting 2nd column\n",
    "ax[1].imshow(output[:, :, 1], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes to all subplots\n",
    "for i in range(2):\n",
    "    ax[i].axes.xaxis.set_ticks([])\n",
    "    ax[i].axes.yaxis.set_ticks([])\n",
    "\n",
    "\n",
    "# Giving names to subplots along X axis\n",
    "ax[0].set_xlabel('Laplacian', fontsize=18)\n",
    "ax[1].set_xlabel('Sobel', fontsize=18)\n",
    "\n",
    "\n",
    "# Moving subplots names to the top positions\n",
    "ax[0].xaxis.set_label_position('top')\n",
    "ax[1].xaxis.set_label_position('top')\n",
    "\n",
    "\n",
    "# Adjusting distance between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Visualize final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Sub-Content:**  \n",
    "  \n",
    "ðŸ’  4.6.**1** **Initialize** 2D Convolutional Layer by Tensorflow for GRAY and RGB input and set of filters  \n",
    "ðŸ’  4.6.**2** **Pass** GRAY and RGB input through Tensorflow Conv2D layer  \n",
    "ðŸ’  4.6.**3** **Visualize** obtained feature maps for GRAY and RGB input  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 Initialize 2D Convolutional Layer by Tensorflow for GRAY and RGB input and set of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for GRAY input and set of filters\n",
    "layer_for_GRAY_input = tf.keras.layers.Conv2D(filters=2,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_GRAY.shape[1:],\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f11))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for RGB input and set of filters\n",
    "layer_for_RGB_input = tf.keras.layers.Conv2D(filters=2,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_RGB.shape[1:],\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f22))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 Pass GRAY and RGB input through Tensorflow Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the Conv2D layer with new weights\n",
    "output_from_GRAY_input = layer_for_GRAY_input(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature maps\n",
    "# Converting output Tensor into Numpy array\n",
    "output_from_GRAY_input = np.array(output_from_GRAY_input[0, :, :, :])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output_from_GRAY_input = np.clip(output_from_GRAY_input, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output_from_GRAY_input.shape)\n",
    "print('\\n' + 'Feature maps are successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing RGB input to the Conv2D layer with new weights\n",
    "output_from_RGB_input = layer_for_RGB_input(x_input_RGB)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature maps\n",
    "# Converting output Tensor into Numpy array\n",
    "output_from_RGB_input = np.array(output_from_RGB_input[0, :, :, :])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output_from_RGB_input = np.clip(output_from_RGB_input, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output_from_RGB_input.shape)\n",
    "print('\\n' + 'Feature maps are successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3 Visualize obtained feature maps for GRAY and RGB input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.5)\n",
    "\n",
    "\n",
    "# Defining a figure object with number of needed subplots\n",
    "figure, ax = plt.subplots(nrows=2, ncols=2)\n",
    "# ax is a (2, 2) Numpy array and to access specific subplot we call it by ax[0, 0]\n",
    "\n",
    "\n",
    "# Adjusting 1st column\n",
    "ax[0, 0].imshow(output_from_GRAY_input[:, :, 0], cmap=plt.get_cmap('gray'))\n",
    "ax[1, 0].imshow(output_from_RGB_input[:, :, 0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Adjusting 2nd column\n",
    "ax[0, 1].imshow(output_from_GRAY_input[:, :, 1], cmap=plt.get_cmap('gray'))\n",
    "ax[1, 1].imshow(output_from_RGB_input[:, :, 1], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes to all subplots\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax[i, j].axes.xaxis.set_ticks([])\n",
    "        ax[i, j].axes.yaxis.set_ticks([])\n",
    "\n",
    "\n",
    "# Giving names to X & Y axes\n",
    "ax[0, 0].set_xlabel('Laplacian', fontsize=18)\n",
    "ax[0, 0].xaxis.set_label_position('top')\n",
    "ax[0, 1].set_xlabel('Sobel', fontsize=18)\n",
    "ax[0, 1].xaxis.set_label_position('top')\n",
    "\n",
    "ax[0, 0].set_ylabel('GRAY         \\n input          ', fontsize=18, rotation='horizontal')\n",
    "ax[1, 0].set_ylabel('RGB         \\n input          ', fontsize=18, rotation='horizontal')\n",
    "\n",
    "\n",
    "# Adjusting distance between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Saving the plot\n",
    "# (!) On Windows, the path might look like following:\n",
    "# r'images\\2d_convolution_Tensorflow.png'\n",
    "# or:\n",
    "# 'images\\\\2d_convolution_Tensorflow.png'\n",
    "figure.savefig('images/2d_convolution_Tensorflow.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ðŸ”˜ Step 5: Apply 2D convolution by Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Content:**  \n",
    "  \n",
    "ðŸ’  5.**1** **Set** up weights for GRAY and RGB input  \n",
    "ðŸ’  5.**2** **Initialize** 2D Convolutional Layer by Keras for GRAY and RGB input and set of filters  \n",
    "ðŸ’  5.**3** **Display** built CNN models  \n",
    "ðŸ’  5.**4** **Pass** GRAY and RGB input through Keras Conv2D layer  \n",
    "ðŸ’  5.**5** **Visualize** obtained feature maps for GRAY and RGB input  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Set up weights for GRAY and RGB input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing shape of the pre-defined GRAY set of filters\n",
    "print(f11.shape)\n",
    "\n",
    "\n",
    "# Showing shape of the pre-defined RGB set of filters\n",
    "print(f22.shape)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Initialize 2D Convolutional Layer by Keras for GRAY and RGB input and set of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Keras model for GRAY input and set of filters by class 'Sequential()'\n",
    "# Other option is to initialize Keras model by class 'Model()'\n",
    "model_GRAY = Sequential()\n",
    "\n",
    "\n",
    "# Adding Conv2D layer to the model for GRAY input and set of filters\n",
    "model_GRAY.add(Conv2D(filters=2,\n",
    "                      kernel_size=(3, 3),\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      input_shape=x_input_GRAY.shape[1:],\n",
    "                      use_bias=False,\n",
    "                      weights=[f11]))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Keras model with Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Keras model for RGB input and set of filters by class 'Sequential()'\n",
    "# Other option is to initialize Keras model by class 'Model()'\n",
    "model_RGB = Sequential()\n",
    "\n",
    "\n",
    "# Adding Conv2D layer to the model for RGB input and set of filters\n",
    "model_RGB.add(Conv2D(filters=2,\n",
    "                     kernel_size=(3, 3),\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=x_input_RGB.shape[1:],\n",
    "                     use_bias=False,\n",
    "                     weights=[f22]))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Keras model with Conv2D layer is successfully initialized ' + '\\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Display built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing GRAY model's summary in form of table\n",
    "model_GRAY.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing RGB model's summary in form of table\n",
    "model_RGB.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting GRAY model's layers in form of flowchart\n",
    "plot_model(model_GRAY,\n",
    "           show_shapes=True,\n",
    "           show_layer_names=False,\n",
    "           rankdir='LR',\n",
    "           dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting RGB model's layers in form of flowchart\n",
    "plot_model(model_RGB,\n",
    "           show_shapes=True,\n",
    "           show_layer_names=False,\n",
    "           rankdir='LR',\n",
    "           dpi=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Pass GRAY and RGB input through Keras Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing GRAY input to the Conv2D layer\n",
    "output_from_GRAY_input = model_GRAY.predict(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature maps\n",
    "# Converting output Tensor into Numpy array\n",
    "output_from_GRAY_input = np.array(output_from_GRAY_input[0, :, :, :])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output_from_GRAY_input = np.clip(output_from_GRAY_input, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output_from_GRAY_input.shape)\n",
    "print('\\n' + 'Feature maps are successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing RGB input to the Conv2D layer\n",
    "output_from_RGB_input = model_RGB.predict(x_input_RGB)\n",
    "\n",
    "\n",
    "# Slicing from the output just feature maps\n",
    "# Converting output Tensor into Numpy array\n",
    "output_from_RGB_input = np.array(output_from_RGB_input[0, :, :, :])\n",
    "\n",
    "\n",
    "# Excluding non-needed values (less than 0 and more than 255)\n",
    "output_from_RGB_input = np.clip(output_from_RGB_input, 0, 255)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('\\n' + 'Shape of the output is :', output_from_RGB_input.shape)\n",
    "print('\\n' + 'Feature maps are successfully sliced from the output')\n",
    "print('\\n' + 'Non-needed values are successfully excluded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting type of the container that holds layers for the entire model\n",
    "print('Datatype of the container that holds layers is  :', type(model_GRAY.layers))  # list\n",
    "print('Datatype of the container that holds layers is  :', type(model_RGB.layers))   # list\n",
    "print()\n",
    "\n",
    "\n",
    "# Getting length of the list that holds layers for the entire model\n",
    "print('Lengths of the list that holds layers is        :', len(model_GRAY.layers))  # 1\n",
    "print('Lengths of the list that holds layers is        :', len(model_RGB.layers))   # 1\n",
    "print()\n",
    "\n",
    "\n",
    "# Getting type of the container that holds weights for 2D convolutional layer\n",
    "print('Datatype of the container that holds weights is :', type(model_GRAY.layers[0].get_weights()))\n",
    "print('Datatype of the container that holds weights is :', type(model_RGB.layers[0].get_weights()))\n",
    "print()\n",
    "\n",
    "\n",
    "# Getting lengths of the list that holds weights for 2D convolutional layer\n",
    "print('Lengths of the list that holds weights is       :', len(model_GRAY.layers[0].get_weights()))\n",
    "print('Lengths of the list that holds weights is       :', len(model_RGB.layers[0].get_weights()))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shape of the weights\n",
    "print(model_GRAY.layers[0].get_weights()[0].shape)  # (3, 3, 1, 2)\n",
    "print(model_RGB.layers[0].get_weights()[0].shape)   # (3, 3, 3, 2)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing one channel of the weights themselves\n",
    "print(model_GRAY.layers[0].get_weights()[0][:, :, 0, 0])\n",
    "print()\n",
    "print(model_RGB.layers[0].get_weights()[0][:, :, 0, 0])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up new weights for GRAY model\n",
    "model_GRAY.layers[0].set_weights([f11])\n",
    "\n",
    "\n",
    "\n",
    "# Setting up new weights for RGB model\n",
    "model_RGB.layers[0].set_weights([f22])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Visualize obtained feature maps for GRAY and RGB input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.5)\n",
    "\n",
    "\n",
    "# Defining a figure object with number of needed subplots\n",
    "figure, ax = plt.subplots(nrows=2, ncols=2)\n",
    "# ax is a (2, 2) Numpy array and to access specific subplot we call it by ax[0, 0]\n",
    "\n",
    "\n",
    "# Adjusting 1st column\n",
    "ax[0, 0].imshow(output_from_GRAY_input[:, :, 0], cmap=plt.get_cmap('gray'))\n",
    "ax[1, 0].imshow(output_from_RGB_input[:, :, 0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Adjusting 2nd column\n",
    "ax[0, 1].imshow(output_from_GRAY_input[:, :, 1], cmap=plt.get_cmap('gray'))\n",
    "ax[1, 1].imshow(output_from_RGB_input[:, :, 1], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "# Hiding axes to all subplots\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax[i, j].axes.xaxis.set_ticks([])\n",
    "        ax[i, j].axes.yaxis.set_ticks([])\n",
    "\n",
    "\n",
    "# Giving names to X & Y axes\n",
    "ax[0, 0].set_xlabel('Laplacian', fontsize=18)\n",
    "ax[0, 0].xaxis.set_label_position('top')\n",
    "ax[0, 1].set_xlabel('Sobel', fontsize=18)\n",
    "ax[0, 1].xaxis.set_label_position('top')\n",
    "\n",
    "ax[0, 0].set_ylabel('GRAY         \\n input          ', fontsize=18, rotation='horizontal')\n",
    "ax[1, 0].set_ylabel('RGB         \\n input          ', fontsize=18, rotation='horizontal')\n",
    "\n",
    "\n",
    "# Adjusting distance between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Saving the plot\n",
    "# (!) On Windows, the path might look like following:\n",
    "# r'images\\2d_convolution_Keras.png'\n",
    "# or:\n",
    "# 'images\\\\2d_convolution_Keras.png'\n",
    "figure.savefig('images/2d_convolution_Keras.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # â±ï¸ Step 6: Compute time spent for 2D convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“œ **Content:**  \n",
    "  \n",
    "ðŸ’  6.**1** Time spent for 2D convolution by **'for' loop**  \n",
    "ðŸ’  6.**2** Time spent for 2D convolution by **Tensorflow**  \n",
    "ðŸ’  6.**3** Time spent for 2D convolution by **Keras**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Time spent for 2D convolution by 'for' loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying to input GRAY image pad frame\n",
    "image_GRAY_pad = np.pad(image_GRAY, (1, 1), mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "# Preparing zero valued array for convolved output image\n",
    "output = np.zeros(image_GRAY.shape)\n",
    "\n",
    "\n",
    "# Measuring spent time\n",
    "# Start point\n",
    "start = timer()\n",
    "\n",
    "\n",
    "# Implementing convolution operation to GRAY image\n",
    "# Sliding through entire input image (that is with pad frame) by Sobel filter\n",
    "# Wrapping the loop with 'tqdm' in order to see progress in Real Time\n",
    "for i in tqdm(range(image_GRAY_pad.shape[0] - 2)):\n",
    "    for j in range(image_GRAY_pad.shape[1] - 2):\n",
    "        # Extracting (slicing) a 3x3 patch (the same size with filter)\n",
    "        # from input image with pad frame\n",
    "        patch = image_GRAY_pad[i:i+3, j:j+3]\n",
    "\n",
    "        # Applying elementwise multiplication and summation -\n",
    "        # this is convolution operation\n",
    "        # When we use '*' with matrices, then elementwise multiplication\n",
    "        # will be applied\n",
    "\n",
    "        # With Sobel filter\n",
    "        output[i, j] = np.sum(patch * f1)\n",
    "\n",
    "        \n",
    "# Measuring spent time\n",
    "# End point\n",
    "end = timer()\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing computed time\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('Time : {0:.5f} seconds'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Time spent for 2D convolution by Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Initializing Conv2D layer for GRAY input\n",
    "layer = tf.keras.layers.Conv2D(filters=1,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=x_input_GRAY.shape[1:],\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f1))\n",
    "\n",
    "\n",
    "# Measuring spent time\n",
    "# Start point\n",
    "start = timer()\n",
    "\n",
    "\n",
    "# Passing GRAY input to the Tensorflow Conv2D layer\n",
    "output = layer(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Measuring spent time\n",
    "# End point\n",
    "end = timer()\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing computed time\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('Time : {0:.5f} seconds'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Time spent for 2D convolution by Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping filter to get following: (height, width, channels, filters)\n",
    "f_new = f1.reshape(3, 3, 1, 1)\n",
    "\n",
    "\n",
    "# Initializing Keras model for GRAY input and set of filters\n",
    "model_GRAY = Sequential()\n",
    "\n",
    "\n",
    "# Adding Conv2D layer to the model for GRAY input\n",
    "model_GRAY.add(Conv2D(filters=1,\n",
    "                      kernel_size=(3, 3),\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      input_shape=x_input_GRAY.shape[1:],\n",
    "                      use_bias=False,\n",
    "                      weights=[f_new]))\n",
    "\n",
    "\n",
    "# Measuring spent time\n",
    "# Start point\n",
    "start = timer()\n",
    "\n",
    "\n",
    "# Passing GRAY input to the Keras Conv2D layer\n",
    "output = model_GRAY.predict(x_input_GRAY)\n",
    "\n",
    "\n",
    "# Measuring spent time\n",
    "# End point\n",
    "end = timer()\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing computed time\n",
    "print('Convolution is successfully applied ' + '\\U0001F44C')\n",
    "print('Time : {0:.5f} seconds'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“½ï¸ Step 7: Implement 2D convolution on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using environment for GPU, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, try out following options:\n",
    "# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n",
    "# Option-2: switch to the environment for CPU usage only instead of GPU\n",
    "\n",
    "\n",
    "# Defining 'VideoCapture' object and reading video from a file\n",
    "# (!) On Windows, the path might look like following:\n",
    "# r'videos\\polar_bear.mp4'\n",
    "# or:\n",
    "# 'videos\\\\polar_bear.mp4'\n",
    "video = cv2.VideoCapture('videos/polar_bear.mp4')\n",
    "\n",
    "\n",
    "# Preparing variables for writers that will be used to write processed frames into video files\n",
    "writer_BGR = None\n",
    "writer_GRAY = None\n",
    "\n",
    "\n",
    "# Preparing variables for spatial dimensions of the captured frames\n",
    "h, w = None, None\n",
    "\n",
    "\n",
    "# Getting version of OpenCV that is currently used\n",
    "# Converting string into the list by dot as separator and getting first number\n",
    "v = cv2.__version__.split('.')[0]\n",
    "\n",
    "\n",
    "# Defining variable for counting frames\n",
    "# At the end we will show total amount of processed frames\n",
    "f = 0\n",
    "\n",
    "\n",
    "# Defining variable for calculating total spent time\n",
    "# At the end we will show time spent for processing all frames\n",
    "t = 0\n",
    "\n",
    "\n",
    "# Defining loop for capturing frames\n",
    "while True:\n",
    "    # Capturing frame-by-frame\n",
    "    ret, frame_BGR = video.read()\n",
    "\n",
    "    # If the frame was not retrieved\n",
    "    # e.g.: at the end of the video,\n",
    "    # then we break the loop\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Getting spatial dimension of the frame\n",
    "    # We do it only once from the very beginning\n",
    "    # All other frames have the same dimensions\n",
    "    if w is None or h is None:\n",
    "        # Getting shape of the caught frame\n",
    "        (h, w, c) = frame_BGR.shape\n",
    "        \n",
    "        \n",
    "        # Initializing Conv2D layer for RGB input\n",
    "        # We do it only once from the very beginning\n",
    "        layer = tf.keras.layers.Conv2D(filters=1,\n",
    "                                       kernel_size=(3, 3),\n",
    "                                       strides=1,\n",
    "                                       padding='same',\n",
    "                                       activation='relu',\n",
    "                                       input_shape=(h, w, c),\n",
    "                                       use_bias=False,\n",
    "                                       kernel_initializer=tf.keras.initializers.constant(f1_3))\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Implementing 2D convolution to the captured frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converting captured frame to RGB by OpenCV function\n",
    "    frame_RGB = cv2.cvtColor(frame_BGR, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    # Reshaping frame to get following: (batch size, rows, columns, channels)\n",
    "    x_input_RGB = frame_RGB.reshape(1, h, w, c).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    # Passing RGB input to the initialized Conv2D layer\n",
    "    # Calculating time spent for 2D convolution\n",
    "    start = timer()\n",
    "    output = layer(x_input_RGB)\n",
    "    end = timer()\n",
    "    \n",
    "    \n",
    "    # Slicing from the output just feature map\n",
    "    # Converting output Tensor into Numpy array\n",
    "    output = np.array(output[0, :, :, 0])\n",
    "    \n",
    "    \n",
    "    # To exclude values that are less than 0 and more than 255,\n",
    "    # Numpy function 'clip' is applied\n",
    "    # It keeps values of Numpy array in the given range\n",
    "    # And it replaces non-needed values with boundary numbers\n",
    "    output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    # Increasing counters for frames and total spent time\n",
    "    f += 1\n",
    "    t += end - start\n",
    "    \n",
    "    \n",
    "    # Showing spent time for single current frame\n",
    "    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Implementing 2D convolution to the captured frame\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Finding contours\n",
    "    \"\"\"\n",
    "    \n",
    "    # Finding contours\n",
    "    # (!) Different versions of OpenCV returns different number of parameters\n",
    "    # when using function cv2.findContours()\n",
    "\n",
    "    # In OpenCV version 3 function cv2.findContours() returns three parameters:\n",
    "    # modified image, found contours and hierarchy\n",
    "    # All found contours from current frame are stored in the list\n",
    "    # Each individual contour is a Numpy array of(x, y) coordinates\n",
    "    # of the boundary points of the object\n",
    "    # We are interested only in contours\n",
    "\n",
    "    # Checking if OpenCV version 3 is used\n",
    "    if v == '3':\n",
    "        _, contours, _ = cv2.findContours(output, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # In OpenCV version 4 function cv2.findContours() returns two parameters:\n",
    "    # found contours and hierarchy\n",
    "    # All found contours from current frame are stored in the list\n",
    "    # Each individual contour is a Numpy array of(x, y) coordinates\n",
    "    # of the boundary points of the object\n",
    "    # We are interested only in contours\n",
    "\n",
    "    # Checking if OpenCV version 4 is used\n",
    "    else:\n",
    "        contours, _ = cv2.findContours(output, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Finding the biggest contour by sorting from biggest to smallest\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Finding contours\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Drawing bounding box and label\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting coordinates of the biggest contour if any was found\n",
    "    if contours:\n",
    "        # Getting rectangle coordinates and spatial dimension of the biggest contour\n",
    "        # Function cv2.boundingRect() is used to get an approximate rectangle\n",
    "        # around the region of interest in the binary image after contour was found\n",
    "        (x_min, y_min, box_width, box_height) = cv2.boundingRect(contours[0])\n",
    "        \n",
    "        \n",
    "        # Drawing bounding box on the current BGR frame\n",
    "        cv2.rectangle(frame_BGR,\n",
    "                      (x_min, y_min),\n",
    "                      (x_min + box_width, y_min + box_height),\n",
    "                      (0, 255, 0),\n",
    "                      3)\n",
    "        \n",
    "        \n",
    "        # Preparing text for the label\n",
    "        label = 'Polar Bear'\n",
    "        \n",
    "        \n",
    "        # Putting text with label on the current BGR frame\n",
    "        cv2.putText(frame_BGR,\n",
    "                    label,\n",
    "                    (x_min - 5, y_min - 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.5,\n",
    "                    (0, 255, 0),\n",
    "                    2)        \n",
    "        \n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Drawing bounding box and label\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Writing processed frames into the files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing writer for BGR frame\n",
    "    # we do it only once from the very beginning\n",
    "    # when we get spatial dimensions of the frames\n",
    "    if writer_BGR is None:\n",
    "        # Constructing code of the codec\n",
    "        # to be used in the function VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        # Writing current processed BGR frame into the video file\n",
    "        # (!) On Windows, the path might look like following:\n",
    "        # r'videos\\result_RGB.mp4'\n",
    "        # or:\n",
    "        # 'videos\\\\result_RGB.mp4'\n",
    "        writer_BGR = cv2.VideoWriter('videos/result_RGB.mp4',\n",
    "                                     fourcc,\n",
    "                                     30,\n",
    "                                     (w, h),\n",
    "                                     True)\n",
    "        \n",
    "        \n",
    "    # Initializing writer for GRAY output\n",
    "    # we do it only once from the very beginning\n",
    "    # when we get spatial dimensions of the frames\n",
    "    if writer_GRAY is None:\n",
    "        # Constructing code of the codec\n",
    "        # to be used in the function VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        # Writing current processed GRAY output into the video file\n",
    "        # (!) On Windows, the path might look like following:\n",
    "        # r'videos\\result_GRAY.mp4'\n",
    "        # or:\n",
    "        # 'videos\\\\result_GRAY.mp4'\n",
    "        writer_GRAY = cv2.VideoWriter('videos/result_GRAY.mp4',\n",
    "                                      fourcc,\n",
    "                                      30,\n",
    "                                      (w, h),\n",
    "                                      False)\n",
    "        \n",
    "        \n",
    "    # Write processed current BGR frame to the file\n",
    "    writer_BGR.write(frame_BGR)\n",
    "    \n",
    "    # Write processed current GRAY output to the file\n",
    "    writer_GRAY.write(output)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Writing processed frames into the files\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "# Printing final results\n",
    "print()\n",
    "print('Total number of frames is              :', f)\n",
    "print('Total spent time for 2D convolution is : {:.5f} seconds'.format(t))\n",
    "print('FPS rate is                            :', round((f / t), 1))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Releasing video reader and writers\n",
    "video.release()\n",
    "writer_BGR.release()\n",
    "writer_GRAY.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—’ï¸ Some comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ”ï¸ To get more details for usage of ***Numpy functions:***  \n",
    "> print(help(**np.copy**))  \n",
    "> print(help(**np.clip**))  \n",
    "  \n",
    "ðŸ”— More details and examples are here:  \n",
    "> https://numpy.org/doc/stable/reference/generated/numpy.copy.html  \n",
    "> https://numpy.org/devdocs/reference/generated/numpy.clip.html  \n",
    "  \n",
    "  <br/>\n",
    "  \n",
    "âœ”ï¸ To get more details for usage of ***Tensorflow:***  \n",
    "> print(help(**tf.keras.layers.Conv2D**))  \n",
    "  \n",
    "ðŸ”— More details and examples are here:  \n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D  \n",
    "  \n",
    "  <br/>\n",
    "  \n",
    "âœ”ï¸ To get more details for usage of ***Keras:***  \n",
    "> print(help(**Sequential**))  \n",
    "> print(help(**Conv2D**))  \n",
    "> print(help(**plot_model**))  \n",
    "> print(help(**Sequential.get_weights**))  \n",
    "> print(help(**Sequential.set_weights**))  \n",
    "> print(help(**Sequential.load_weights**))  \n",
    "> print(help(**Sequential.predict**))  \n",
    "  \n",
    "ðŸ”— More details and examples are here:  \n",
    "> https://keras.io/api/models/sequential/  \n",
    "> https://keras.io/api/layers/convolution_layers/convolution2d/  \n",
    "> https://keras.io/api/utils/model_plotting_utils/#plot_model-function  \n",
    "  \n",
    "  <br/>\n",
    "  \n",
    "âœ”ï¸ To get more details for usage of ***OpenCV functions:***  \n",
    "> print(help(**cv2.VideoCapture**))  \n",
    "> print(help(**cv2.findContours**))  \n",
    "> print(help(**cv2.rectangle**))  \n",
    "> print(help(**cv2.putText**))  \n",
    "> print(help(**cv2.VideoWriter_fourcc**))  \n",
    "> print(help(**cv2.VideoWriter**))  \n",
    "  \n",
    "ðŸ”— More details and examples are here:  \n",
    "> https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html  \n",
    "> https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html  \n",
    "> https://docs.opencv.org/4.0.0/d4/d73/tutorial_py_contours_begin.html  \n",
    "> https://docs.opencv.org/master/dc/da5/tutorial_py_drawing_functions.html  \n",
    "> https://docs.opencv.org/3.4/dd/d9e/classcv_1_1VideoWriter.html  \n",
    "> http://www.fourcc.org  \n",
    "  \n",
    "  <br/>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print(help(np.copy))\n",
    "print('\\n')\n",
    "print(help(np.clip))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(help(tf.keras.layers.Conv2D))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "print(help(Sequential))\n",
    "print('\\n')\n",
    "print(help(Conv2D))\n",
    "print('\\n')\n",
    "print(help(plot_model))\n",
    "print('\\n')\n",
    "print(help(Sequential.get_weights))\n",
    "print('\\n')\n",
    "print(help(Sequential.set_weights))\n",
    "print('\\n')\n",
    "print(help(Sequential.load_weights))\n",
    "print('\\n')\n",
    "print(help(Sequential.predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "print(help(cv2.VideoCapture))\n",
    "print('\\n')\n",
    "print(help(cv2.findContours))\n",
    "print('\\n')\n",
    "print(help(cv2.rectangle))\n",
    "print('\\n')\n",
    "print(help(cv2.putText))\n",
    "print('\\n')\n",
    "print(help(cv2.VideoWriter_fourcc))\n",
    "print('\\n')\n",
    "print(help(cv2.VideoWriter))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prewitt filter to detect vertical changes on image\n",
    "f3 = np.array([[1, 0, -1], \n",
    "               [1, 0, -1], \n",
    "               [1, 0, -1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prewitt filter to detect horizontal changes on image\n",
    "f4 = np.array([[1, 1, 1],  \n",
    "               [0, 0, 0],  \n",
    "               [-1, -1, -1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobel filter to detect horizontal changes on image\n",
    "f5 = np.array([[1, 2, 1],  \n",
    "               [0, 0, 0],  \n",
    "               [-1, -2, -1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian blur filter\n",
    "f6 = (1 / 16) * np.array([[1, 2, 1],\n",
    "                          [2, 4, 2],\n",
    "                          [1, 2, 1]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
